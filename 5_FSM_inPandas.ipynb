{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import graphlab as gl\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import src.metadata_handler as mdh\n",
    "import src.clean_db as cdb\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_df = pd.read_csv('data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_df = raw_df.rename(columns = {'Unnamed: 0':'photo_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = raw_df[['keywords', 'photo_id','date_created','time_created','copyright_notice','file_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.copyright_notice.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH LOCAL PHOTOS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create my dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "0\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "10\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "20\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "30\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "30\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "30\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "30\n",
      "('WARNING: problems with charset recognition', \"'\\\\x1b'\")\n",
      "30\n",
      "0 photos failed to process\n"
     ]
    }
   ],
   "source": [
    "mdh.build_json_database('first_sample','data/testing/first_sampl.json')\n",
    "cdb.main('data/testing/first_sampl.json', 'data/testing/first_sampl.csv')\n",
    "\n",
    "raw_df = pd.read_csv('data/testing/first_sampl.csv')\n",
    "# raw_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>photo_id</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copyright_notice</th>\n",
       "      <td>Conservation Northwest</td>\n",
       "      <td>not_provided</td>\n",
       "      <td>Conservation Northwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_created</th>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_path</th>\n",
       "      <td>first_sample/EK000004-2.JPG</td>\n",
       "      <td>first_sample/EK000004.JPG</td>\n",
       "      <td>first_sample/EK000010-2.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_created</th>\n",
       "      <td>11:49:52</td>\n",
       "      <td>None</td>\n",
       "      <td>14:54:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[]</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[u'Camera Check']</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[u'hoary marmot']</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[u'mule deer']</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[u'unidentified']</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0                          1  \\\n",
       "photo_id                                     0                          1   \n",
       "copyright_notice        Conservation Northwest               not_provided   \n",
       "date_created                        2015-05-30                       None   \n",
       "file_path          first_sample/EK000004-2.JPG  first_sample/EK000004.JPG   \n",
       "time_created                          11:49:52                       None   \n",
       "[]                                           0                          0   \n",
       "[u'Camera Check']                            1                          0   \n",
       "[u'hoary marmot']                            0                          0   \n",
       "[u'mule deer']                               0                          0   \n",
       "[u'unidentified']                            0                          0   \n",
       "\n",
       "                                             2  \n",
       "photo_id                                     2  \n",
       "copyright_notice        Conservation Northwest  \n",
       "date_created                        2015-06-04  \n",
       "file_path          first_sample/EK000010-2.JPG  \n",
       "time_created                          14:54:40  \n",
       "[]                                           0  \n",
       "[u'Camera Check']                            0  \n",
       "[u'hoary marmot']                            1  \n",
       "[u'mule deer']                               0  \n",
       "[u'unidentified']                            0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_df.rename(columns = {'Unnamed: 0':'photo_id'})\n",
    "# df['image'] = image_pdSeries <-- this turns images into dicts (can't change them back)\n",
    "\n",
    "# add dummy cols to dataframe:\n",
    "df = pd.concat([df, pd.get_dummies(df.keywords)], axis=1)\n",
    "\n",
    "df.loc[pd.isnull(df.copyright_notice), 'copyright_notice'] = \"not_provided\"\n",
    "\n",
    "# Drop unneeded columns\n",
    "df = df.drop(['by_line', 'caption_abstract',\n",
    "              'contact','object_name','sub_location',\n",
    "              'supplemental_category','keywords', \"[u'camera check']\", \"[u'marmot']\", \"[u'deer']\"],\n",
    "             axis = 1)\n",
    "\n",
    "# # take only rows where copyright is properly entered\n",
    "# df = df.loc[df['copyright_notice'] == 'Conservation Northwest']\n",
    "\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play with graphlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphlab example of feature extraction:\n",
    "```python\n",
    ">>> data = graphlab.SFrame('https://static.turi.com/datasets/mnist/sframe/train6k')\n",
    ">>> net = graphlab.deeplearning.get_builtin_neuralnet('mnist')\n",
    ">>> m = graphlab.neuralnet_classifier.create(data,\n",
    "...                                          target='label',\n",
    "...                                          network=net,\n",
    "...                                          max_iterations=3)\n",
    ">>> # Now, let's extract features from the last layer\n",
    ">>> data['features'] = m.extract_features(data)\n",
    ">>> # Now, let's build a new classifier on top of extracted features\n",
    ">>> m = graphlab.classifier.create(data,\n",
    "...                                          features = ['features'],\n",
    "...                                          target='label')\n",
    "```\n",
    "Now, let’s see how to load the ImageNet model, and use it for extracting features after resizing the data:\n",
    "```python\n",
    "\n",
    ">>> imagenet_model = graphlab.load_model('https://static.turi.com/products/graphlab-create/resources/models/python2.7/imagenet_model_iter45')\n",
    ">>> data['image'] = graphlab.image_analysis.resize(data['image'], 256, 256, 3, decode=True)\n",
    ">>> data['imagenet_features'] = imagenet_model.extract_features(data)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SFrame.dtype of Columns:\n",
       "\tlabel\tint\n",
       "\timage\tImage\n",
       "\n",
       "Rows: 6000\n",
       "\n",
       "Data:\n",
       "+-------+----------------------+\n",
       "| label |        image         |\n",
       "+-------+----------------------+\n",
       "|   5   | Height: 28 Width: 28 |\n",
       "|   8   | Height: 28 Width: 28 |\n",
       "|   1   | Height: 28 Width: 28 |\n",
       "|   4   | Height: 28 Width: 28 |\n",
       "|   2   | Height: 28 Width: 28 |\n",
       "|   7   | Height: 28 Width: 28 |\n",
       "|   0   | Height: 28 Width: 28 |\n",
       "|   2   | Height: 28 Width: 28 |\n",
       "|   5   | Height: 28 Width: 28 |\n",
       "|   9   | Height: 28 Width: 28 |\n",
       "+-------+----------------------+\n",
       "[6000 rows x 2 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = gl.SFrame('https://static.turi.com/datasets/mnist/sframe/train6k')\n",
    "\n",
    "image_col = data2['image']\n",
    "\n",
    "data2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn dataframe into SFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = gl.SFrame(df)\n",
    "\n",
    "# GET IMAGES:\n",
    "img_sframe = gl.image_analysis.load_images('first_sample', \"auto\", with_path=False,\n",
    "                                                    recursive=True)\n",
    "\n",
    "# ADD IMAGES TO data\n",
    "data.add_column(img_sframe['image'], name='image')\n",
    "\n",
    "# Filter for desired rows ('copyright_notice' == 'Conservation Northwest')\n",
    "data = data[data['copyright_notice'] == 'Conservation Northwest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Downloading https://static.turi.com/products/graphlab-create                                        /resources/models/python2.7/imagenet_model_iter45/dir_archive.ini to /var/tmp/graphlab-ophidian/33361/629e0223-1cf0-4979-ac68-d4dc1de40ab6.ini</pre>"
      ],
      "text/plain": [
       "Downloading https://static.turi.com/products/graphlab-create                                        /resources/models/python2.7/imagenet_model_iter45/dir_archive.ini to /var/tmp/graphlab-ophidian/33361/629e0223-1cf0-4979-ac68-d4dc1de40ab6.ini"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Downloading https://static.turi.com/products/graphlab-create                                        /resources/models/python2.7/imagenet_model_iter45/objects.bin to /var/tmp/graphlab-ophidian/33361/1624c76d-3838-4ec7-b2c8-a86e315c40e6.bin</pre>"
      ],
      "text/plain": [
       "Downloading https://static.turi.com/products/graphlab-create                                        /resources/models/python2.7/imagenet_model_iter45/objects.bin to /var/tmp/graphlab-ophidian/33361/1624c76d-3838-4ec7-b2c8-a86e315c40e6.bin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Imagenet pretrained CNN\n",
    "imagenet_model = gl.load_model('https://static.turi.com/products/graphlab-create\\\n",
    "                                        /resources/models/python2.7/imagenet_model_iter45')\n",
    "# Convert photos to proper size for imagenet\n",
    "data['image'] = gl.image_analysis.resize(data['image'], 256, 256, 3, decode=True)\n",
    "\n",
    "# Get features from CNN\n",
    "data['imagenet_features'] = imagenet_model.extract_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true | pred | prob [mule deer = true]\n",
      "0.0    0.0    0.00226344941121\n",
      "0.0    0.0    0.0433000493441\n",
      "0.0    0.0    0.00980105000864\n",
      "0.0    0.0    0.477570801839\n",
      "0.0    0.0    0.498634624133\n",
      "0.0    0.0    0.0618667062038\n",
      "1.0    1.0    0.946675543696\n",
      "1.0    1.0    0.987679561889\n",
      "1.0    0.0    0.498634624133\n",
      "1.0    1.0    0.990925428388\n",
      "0.0    0.0    0.00319358644227\n",
      "0.0    0.0    0.00319358644227\n",
      "1.0    1.0    0.501632769344\n",
      "1.0    0.0    0.477570801839\n",
      "1.0    1.0    0.990925428388\n",
      "0.0    1.0    0.501632769344\n",
      "0.0    0.0    0.00319358644227\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected array-like (array or non-string sequence), got dtype: float\nRows: 17\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-343918f8a5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ophidian/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ophidian/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ophidian/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ophidian/anaconda/lib/python2.7/site-packages/sklearn/utils/multiclass.pyc\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError('Expected array-like (array or non-string sequence), '\n\u001b[0;32m--> 235\u001b[0;31m                          'got %r' % y)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multilabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected array-like (array or non-string sequence), got dtype: float\nRows: 17\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]"
     ]
    }
   ],
   "source": [
    "log_regr = LR(penalty='l2', dual=False, tol=0.0001, C=1.0,\n",
    "           fit_intercept=True, intercept_scaling=1,\n",
    "           class_weight=None, random_state=None, solver='liblinear',\n",
    "           max_iter=100, multi_class='ovr', verbose=0,\n",
    "           warm_start=False, n_jobs=2)\n",
    "\n",
    "X = data['imagenet_features']\n",
    "y = data[\"[u'mule deer']\"]\n",
    "\n",
    "# fit model to data\n",
    "model = log_regr.fit(X,y)\n",
    "pred = model.predict(X)\n",
    "prob = model.predict_proba(X)\n",
    "\n",
    "print'true | pred | prob [mule deer = true]'\n",
    "for a,b,c in zip(y, pred, prob):\n",
    "    print a,'  ',b,'  ',c[1]\n",
    "\n",
    "print model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
